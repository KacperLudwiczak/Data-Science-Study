{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ3UjMaHOvrUWp0JWxWj9R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KacperLudwiczak/Data-Science-Study/blob/main/Lekcja_3_Business_Case.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qzNPcyf9jJ0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "v53jK9WR-HGK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg3Z7jez62E0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "\n",
        "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')\n",
        "\n",
        "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
        "targets_all = raw_csv_data[:,-1]\n",
        "\n",
        "# Ten kod zaimportuje potrzebne biblioteki, wczyta dane z pliku CSV o nazwie 'Audiobooks_data.csv' i przypisze dane do zmiennych. Konkretnie, nieprzeskalowane dane wejściowe są przechowywane w zmiennej \"unscaled_inputs_all\", a cele są przechowywane w zmiennej \"targets_all\". Przed przetwarzaniem danych, zazwyczaj należy przeskalować dane, co zostanie wykonane za pomocą biblioteki sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balance the dataset"
      ],
      "metadata": {
        "id": "Tc4mzx1KkvVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_one_targets = int(np.sum(targets_all))\n",
        "zero_targets_counter = 0\n",
        "indices_to_remove = []\n",
        "\n",
        "for i in range(targets_all.shape[0]):\n",
        "  if targets_all[i] ==0:\n",
        "    zero_targets_counter +=1\n",
        "    if zero_targets_counter > num_one_targets:\n",
        "      indices_to_remove.append(i)\n",
        "\n",
        "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)\n",
        "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)\n",
        "\n",
        "# Ten kod ma na celu zrównoważenie liczby danych wejściowych dla dwóch klas. Pierwsza pętla oblicza ilość celów, które mają wartość 1, a następnie oblicza ilość celów, które mają wartość 0, przechodząc po wszystkich celach. Następnie oblicza indeksy danych wejściowych, które należy usunąć z niezrównoważonych danych, aby uzyskać zrównoważone dane wejściowe.\n",
        "# W drugiej pętli, każdy indeks danych wejściowych, które mają wartość 0, są usuwane, jeśli przekraczają ilość danych wejściowych, które mają wartość 1. W ten sposób dane wejściowe będą miały równą liczbę przykładów dla obu klas. Skasowane indeksy są zapisywane w zmiennej \"indices_to_remove\".\n",
        "# Na koniec, zmienne \"unscaled_inputs_equal_priors\" i \"targets_equal_priors\" przechowują odpowiednio przeskalowane dane wejściowe i cele, które zostały zrównoważone."
      ],
      "metadata": {
        "id": "ddPOr1dIkbmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardize the inputs"
      ],
      "metadata": {
        "id": "CnAes08JnvYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)\n",
        "\n",
        "# Ten kod przeprowadza standardyzację (znormalizowanie) nieprzeskalowanych danych wejściowych, które zostały zrównoważone pod względem liczby przykładów dla obu klas. Standardyzacja polega na przeskalowaniu każdej cechy (kolumny) danych wejściowych tak, aby miała średnią wartość równą 0 i odchylenie standardowe równe 1.\n",
        "# W tym celu używana jest funkcja preprocessing.scale z biblioteki sklearn. W ten sposób, scaled_inputs przechowuje przeskalowane dane wejściowe, które są gotowe do użycia w modelu regresji logistycznej (lub innym modelu uczenia maszynowego) do dalszej analizy."
      ],
      "metadata": {
        "id": "ZCp3QD5cn1fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shuffle the data"
      ],
      "metadata": {
        "id": "J2Dpy98PpY9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
        "shuffled_targets = targets_equal_priors[shuffled_indices]\n",
        "\n",
        "# Ten kod tasuje dane wejściowe i odpowiadające im cele, aby uniknąć wpływu jakiejkolwiek struktury, która mogłaby wpłynąć na proces uczenia się modelu.\n",
        "# W pierwszej linii, tworzony jest wektor indeksów, który odpowiada indeksom wierszy w macierzy scaled_inputs. Następnie funkcja np.random.shuffle tasuje te indeksy w losowej kolejności.\n",
        "# W drugiej i trzeciej linii, macierze shuffled_inputs i shuffled_targets są tworzone, aby przechowywać przetasowane dane wejściowe i odpowiadające im cele. Do przetasowania używana jest macierz shuffled_indices, która przechowuje indeksy w losowej kolejności. W ten sposób, indeksy w obu macierzach shuffled_inputs i shuffled_targets odpowiadają teraz innym wierszom niż w macierzy pierwotnej.\n",
        "# Efektem końcowym jest uzyskanie tasowanych danych wejściowych i odpowiadających im celów, które będą używane do treningu i testowania modelu regresji logistycznej (lub innego modelu uczenia maszynowego) w sposób losowy i bez wpływu na istniejącą strukturę danych."
      ],
      "metadata": {
        "id": "PEGVwaTupb3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the dataset into train, validation, and test"
      ],
      "metadata": {
        "id": "avYffjfHqWaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples_count = shuffled_inputs.shape[0]\n",
        "\n",
        "train_samples_count = int(0.8 * samples_count)\n",
        "validation_samples_count = int(0.1 * samples_count)\n",
        "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
        "\n",
        "train_inputs = shuffled_inputs[:train_samples_count]\n",
        "train_targets = shuffled_targets[:train_samples_count]\n",
        "\n",
        "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
        "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
        "\n",
        "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
        "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
        "\n",
        "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
        "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
        "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)\n",
        "\n",
        "# Ten kod dzieli przetasowane dane wejściowe i odpowiadające im cele na trzy zbiory: treningowy, walidacyjny i testowy.\n",
        "# W pierwszych trzech liniach, liczba wszystkich próbek jest przechowywana w samples_count.\n",
        "# W następnej linii, liczba próbek w zbiorze treningowym jest obliczana na podstawie wartości samples_count. Jest to 80% wszystkich próbek, co odpowiada części treningowej.\n",
        "# Następnie, liczba próbek w zbiorze walidacyjnym jest obliczana jako 10% wartości samples_count.\n",
        "# Ostatnia linia oblicza liczbę próbek w zbiorze testowym jako pozostałe próbki.\n",
        "# W kolejnych linijkach, macierze train_inputs i train_targets są tworzone jako pierwsze train_samples_count wierszy z shuffled_inputs i shuffled_targets, odpowiednio.\n",
        "# Macierze validation_inputs i validation_targets są następnie tworzone jako kolejne validation_samples_count wierszy z shuffled_inputs i shuffled_targets, odpowiednio.\n",
        "# Wreszcie, macierze test_inputs i test_targets są tworzone jako pozostałe próbki w shuffled_inputs i shuffled_targets.\n",
        "# Ostatnie trzy linijki kodu drukują sumę etykiet dla każdego zbioru (treningowego, walidacyjnego, testowego) oraz średnią liczbę etykiet pozytywnych w każdym zbiorze. Ta informacja jest przydatna do monitorowania zrównoważenia klas w każdym zbiorze."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B5RH_xGqc1s",
        "outputId": "2b74307d-faf7-4c10-c7b7-a75d1e1d49f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1772.0 3579 0.4951103660240291\n",
            "222.0 447 0.4966442953020134\n",
            "243.0 448 0.5424107142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the three datasets in .npz"
      ],
      "metadata": {
        "id": "T1njWc0wsarw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
        "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
        "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)\n",
        "\n",
        "# Te linie kodu zapisują trzy pary plików wejściowych-wyjściowych za pomocą np.savez(), co umożliwia zapisanie kilku macierzy NumPy w jednym pliku.\n",
        "# Każdy zestaw danych składa się z dwóch macierzy: inputs i targets.\n",
        "# Pierwszy argument funkcji np.savez() określa nazwę pliku, który zostanie utworzony, a pozostałe argumenty określają pary klucz-wartość, gdzie klucz jest nazwą macierzy, a wartość jest samą macierzą.\n",
        "# Tak więc pierwsza linia kodu zapisuje dane treningowe, druga linia kodu zapisuje dane walidacyjne, a trzecia linia kodu zapisuje dane testowe."
      ],
      "metadata": {
        "id": "_9jQPT4oxMEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the relevant libraries"
      ],
      "metadata": {
        "id": "_JfspC9nzUfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "MhzmXVkszcRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "BP_cUzm1zjsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "npz = np.load('Audiobooks_data_train.npz')\n",
        "\n",
        "train_inputs = npz['inputs'].astype(np.float)\n",
        "train_targets = npz['targets'].astype(np.int)\n",
        "\n",
        "npz = np.load('Audiobooks_data_validation.npz')\n",
        "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
        "\n",
        "npz = np.load('Audiobooks_data_test.npz')\n",
        "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
        "\n",
        "# Ten kod ładuje dane z trzech różnych plików, które prawdopodobnie są zestawami treningowymi, walidacyjnymi i testowymi dla projektu związanego z audiobookami. Dane są ładowane z plików .npz, który jest skompresowanym formatem pliku używanym przez NumPy do przechowywania tablic.\n",
        "# Po załadowaniu danych są one przechowywane w zmiennych o opisowych nazwach. Dane treningowe są przechowywane w train_inputs (tablica wartości zmiennoprzecinkowych) i train_targets (tablica wartości całkowitych). Dane walidacyjne są przechowywane w validation_inputs i validation_targets, a dane testowe w test_inputs i test_targets.\n",
        "# Zauważ, że metoda astype jest używana do konwersji wczytanych danych na odpowiednie typy danych (typ zmiennoprzecinkowy dla wejść i typ całkowitoliczbowy dla celów). Może to być konieczne, ponieważ dane były przechowywane w skompresowanym formacie lub były pierwotnie przechowywane w innym typie danych.\n",
        "# Warto zauważyć, że ten kod zakłada, że trzy pliki .npz znajdują się w tym samym katalogu co skrypt i zawierają dane o oczekiwanych nazwach ('inputs' i 'targets'). Jeśli pliki lub dane nie są obecne lub są sformatowane inaczej, kod prawdopodobnie zawiedzie."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeFLiaWUzlNt",
        "outputId": "b785b600-d773-44fc-e729-ad06591f5403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-cb7c10674898>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  train_inputs = npz['inputs'].astype(np.float)\n",
            "<ipython-input-11-cb7c10674898>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  train_targets = npz['targets'].astype(np.int)\n",
            "<ipython-input-11-cb7c10674898>:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
            "<ipython-input-11-cb7c10674898>:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
            "<ipython-input-11-cb7c10674898>:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
            "<ipython-input-11-cb7c10674898>:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "VLbxcfoQ1q7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 10\n",
        "output_size = 2\n",
        "hidden_layer_size = 50\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
        "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 100\n",
        "max_epochs = 100\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "\n",
        "model.fit(train_inputs, # train inputs\n",
        "          train_targets, # train targets\n",
        "          batch_size=batch_size, # batch size\n",
        "          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
        "          callbacks=[early_stopping], # early stopping\n",
        "          validation_data=(validation_inputs, validation_targets), # validation data\n",
        "          verbose = 2 # making sure we get enough information about the training process\n",
        "          )\n",
        "\n",
        "# Ten kod definiuje i trenuje sieć neuronową przy użyciu biblioteki Keras w TensorFlow. Zostaje utworzony model Sequential, który zawiera trzy warstwy - dwie warstwy ukryte z aktywacją relu oraz warstwa wyjściowa z aktywacją softmax. Warstwy ukryte zawierają po 50 neuronów, a rozmiar wejścia i wyjścia wynosi odpowiednio 10 i 2.\n",
        "# Model jest kompilowany z użyciem optymalizatora adam, funkcji straty sparse_categorical_crossentropy i miary jakości accuracy. sparse_categorical_crossentropy jest funkcją straty dla przypadków, gdy cele są przypisane do całkowitych wartości, a nie jako wektory one-hot.\n",
        "# Następnie sieć jest trenowana na danych treningowych (train_inputs i train_targets) z batch size wynoszącym 100, maksymalną liczbą epok wynoszącą 100 i wykorzystaniem wcześniejszego zatrzymania (EarlyStopping) w celu uniknięcia przeuczenia. Dane walidacyjne (validation_inputs i validation_targets) są wykorzystywane do oceny dokładności modelu podczas treningu, a verbose ustawiony na 2 pozwala na wyświetlanie szczegółowych informacji na temat procesu trenowania.\n",
        "# Warto zauważyć, że warstwy ukryte z funkcją aktywacji relu pomagają w ekstrakcji cech z wejścia, a warstwa wyjściowa z funkcją aktywacji softmax dokonuje klasyfikacji na dwie możliwe klasy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzKz9W0c1s5p",
        "outputId": "b2f408c7-7cb7-400e-e73b-6f2414aa68d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 - 1s - loss: 0.5803 - accuracy: 0.6818 - val_loss: 0.5072 - val_accuracy: 0.7315 - 987ms/epoch - 27ms/step\n",
            "Epoch 2/100\n",
            "36/36 - 0s - loss: 0.4654 - accuracy: 0.7527 - val_loss: 0.4464 - val_accuracy: 0.7651 - 123ms/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "36/36 - 0s - loss: 0.4184 - accuracy: 0.7795 - val_loss: 0.4114 - val_accuracy: 0.7897 - 124ms/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "36/36 - 0s - loss: 0.3940 - accuracy: 0.7921 - val_loss: 0.4021 - val_accuracy: 0.7763 - 102ms/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "36/36 - 0s - loss: 0.3784 - accuracy: 0.7966 - val_loss: 0.3934 - val_accuracy: 0.7696 - 111ms/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "36/36 - 0s - loss: 0.3696 - accuracy: 0.7991 - val_loss: 0.3758 - val_accuracy: 0.8076 - 112ms/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "36/36 - 0s - loss: 0.3597 - accuracy: 0.8142 - val_loss: 0.3816 - val_accuracy: 0.7718 - 117ms/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "36/36 - 0s - loss: 0.3535 - accuracy: 0.8187 - val_loss: 0.3654 - val_accuracy: 0.8009 - 109ms/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "36/36 - 0s - loss: 0.3514 - accuracy: 0.8128 - val_loss: 0.3601 - val_accuracy: 0.8054 - 101ms/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "36/36 - 0s - loss: 0.3454 - accuracy: 0.8134 - val_loss: 0.3647 - val_accuracy: 0.8031 - 120ms/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "36/36 - 0s - loss: 0.3448 - accuracy: 0.8148 - val_loss: 0.3574 - val_accuracy: 0.8054 - 110ms/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "36/36 - 0s - loss: 0.3406 - accuracy: 0.8187 - val_loss: 0.3558 - val_accuracy: 0.8009 - 111ms/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "36/36 - 0s - loss: 0.3383 - accuracy: 0.8175 - val_loss: 0.3541 - val_accuracy: 0.7942 - 96ms/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "36/36 - 0s - loss: 0.3382 - accuracy: 0.8181 - val_loss: 0.3504 - val_accuracy: 0.8009 - 107ms/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "36/36 - 0s - loss: 0.3370 - accuracy: 0.8201 - val_loss: 0.3517 - val_accuracy: 0.8076 - 106ms/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "36/36 - 0s - loss: 0.3320 - accuracy: 0.8248 - val_loss: 0.3554 - val_accuracy: 0.8098 - 110ms/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb60bd22e80>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the model"
      ],
      "metadata": {
        "id": "50c1m6WP6NwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n",
        "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))\n",
        "\n",
        "# Ten kod oblicza stratę i dokładność modelu na danych testowych (test_inputs i test_targets) za pomocą metody evaluate z biblioteki Keras i wyświetla wyniki w czytelny sposób. Wartość straty i dokładność są przechowywane w zmiennych test_loss i test_accuracy.\n",
        "# Wywołanie metody evaluate oblicza stratę i dokładność modelu na podstawie danych testowych. Wynik test_loss to wartość funkcji straty dla danych testowych, a test_accuracy to dokładność modelu dla danych testowych. Ostatecznie wynik test_loss i test_accuracy są wyświetlane za pomocą metody print i formatowane z użyciem format."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiCYjgmO6LW1",
        "outputId": "b1f7ccb5-7eb2-4da4-8bc0-3f6bf2b3af07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.7969\n",
            "\n",
            "Test loss: 0.32. Test accuracy: 79.69%\n"
          ]
        }
      ]
    }
  ]
}